{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UT6: EDA - Estadística Descriptiva Aplicada\n",
        "\n",
        "Análisis de datos con Python\n",
        "\n",
        "# UT6: Análisis Exploratorio de Datos (EDA)\n",
        "\n",
        "> **Cuaderno de trabajo — UT6: Análisis Exploratorio de Datos (EDA)**\n",
        ">\n",
        "> Este notebook contiene los bloques de código y ejercicios de la\n",
        "> unidad. Para la teoría completa consulta el libro (PDF).\n",
        ">\n",
        "> **Requisitos:** ejecuta primero la celda de preparación del entorno y\n",
        "> la de carga del *dataset*.\n",
        "\n",
        "## El rol del EDA en el ciclo de vida del proyecto\n",
        "\n",
        "**EDA inicial vs. EDA profundo:** no son lo mismo.\n",
        "\n",
        "-   **EDA inicial:** se realiza *antes* de la limpieza, para\n",
        "    diagnosticar qué problemas tiene el *dataset*.\n",
        "-   **EDA profundo:** se realiza *después* de la limpieza (es el que\n",
        "    aborda esta UT), para descubrir relaciones, patrones y supuestos que\n",
        "    guiarán el modelado.\n",
        "\n",
        "Confundirlos es uno de los errores más frecuentes en proyectos reales.\n",
        "\n",
        "## Las 4 preguntas clave del EDA\n",
        "\n",
        "## Caso de uso: predicción de precios de viviendas\n",
        "\n",
        "**Dataset House Prices (Ames Housing):** Recopilado por Dean De Cock\n",
        "para la ciudad de Ames (Iowa, EE. UU.), contiene **1.460 registros** de\n",
        "ventas de viviendas con **79 variables** que describen prácticamente\n",
        "cada aspecto de la propiedad: superficie, calidad de materiales, año de\n",
        "construcción, número de habitaciones, garaje, sótano, etc. La variable\n",
        "objetivo es `SalePrice` (precio de venta en dólares). Es uno de los\n",
        "*datasets* de referencia en competiciones de regresión de Kaggle y\n",
        "volverá a aparecer en UT7, UT9 y UT10.\n",
        "\n",
        "## Preparación del entorno"
      ],
      "id": "5b65cbac-f0cb-4fbf-925d-5ba94fa670d6"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Librerías necesarias para esta unidad\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "print(f\"Pandas: {pd.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(\"Entorno configurado correctamente\")"
      ],
      "id": "9c0cf256"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparación del *dataset* House Prices\n",
        "\n",
        "**Conexión con UT5**\n",
        "\n",
        "Esta limpieza básica aplica técnicas que viste en la unidad anterior:\n",
        "imputación de nulos con mediana (variables numéricas) y moda\n",
        "(categóricas). Aquí lo hacemos de forma compacta para poder centrarnos\n",
        "en el EDA."
      ],
      "id": "a66af74b-8480-4b13-99a2-db0581093da4"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cargado: 1460 filas x 81 columnas"
          ]
        }
      ],
      "source": [
        "# Dataset Ames Housing: 1.460 viviendas, 79 variables predictoras + Id + SalePrice (81 columnas)\n",
        "url = \"https://raw.githubusercontent.com/jgarcia314/analisis-datos-python-fp/main/data/raw/house_prices.csv\"\n",
        "df = pd.read_csv(url, na_values=[\"NA\"])\n",
        "print(f\"Dataset cargado: {df.shape[0]} filas x {df.shape[1]} columnas\")"
      ],
      "id": "905739f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** El *dataset* contiene 1.460 viviendas y\n",
        "81 columnas (79 variables predictoras + `Id` + `SalePrice`). Con esta\n",
        "cantidad de registros y variables, el EDA cobra especial importancia:\n",
        "sin exploración previa, el riesgo de usar predictores irrelevantes o con\n",
        "problemas no detectados es muy alto. La limpieza básica que sigue a\n",
        "continuación es el paso previo necesario antes de cualquier análisis\n",
        "descriptivo."
      ],
      "id": "c8c202c3-0cc9-4b5d-b45e-c347f8094197"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === LIMPIEZA BÁSICA ===\n",
        "print(\"\\n=== LIMPIEZA BÁSICA ===\")\n",
        "\n",
        "# 1. Inspección inicial de valores nulos\n",
        "print(\"\\n1. Valores nulos por columna:\")\n",
        "nulos = df.isnull().sum()\n",
        "print(nulos[nulos > 0])\n",
        "\n",
        "# 2. Tratamiento de valores nulos\n",
        "# Para columnas numéricas: imputar con la mediana (vectorizado por columna)\n",
        "columnas_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "df[columnas_numericas] = df[columnas_numericas].fillna(df[columnas_numericas].median())\n",
        "\n",
        "# Para columnas categóricas: imputar con la moda (vectorizado por columna)\n",
        "columnas_categoricas = df.select_dtypes(include=['object']).columns\n",
        "df[columnas_categoricas] = df[columnas_categoricas].fillna(\n",
        "    df[columnas_categoricas].mode().iloc[0]\n",
        ")\n",
        "\n",
        "# 3. Verificación\n",
        "print(f\"\\nValores nulos después de limpieza: {df.isnull().sum().sum()}\")\n",
        "print(f\"\\nDataset listo para EDA: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "print(f\"Variable objetivo: SalePrice\")"
      ],
      "id": "f3993771"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Documentación del EDA**\n",
        "\n",
        "En proyectos profesionales, siempre documenta tu EDA en un Jupyter\n",
        "Notebook separado. Esto permite que otros (y tú mismo en el futuro)\n",
        "entiendan las decisiones que tomaste basándote en los datos.\n",
        "\n",
        "## Análisis univariante: entendiendo variable por variable\n",
        "\n",
        "### Primera inspección del *dataset*"
      ],
      "id": "d4996bb2-7b1b-4ab0-aa9d-53d3101674d9"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== INSPECCIÓN DEL DATASET ===\")\n",
        "\n",
        "# El dataset ya está cargado y limpio de la sección anterior\n",
        "print(f\"Forma del *dataset*: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "print(\"\\nPrimeras filas:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nInformación general:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nTipos de variables:\")\n",
        "print(df.dtypes.value_counts())"
      ],
      "id": "747dc836"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables numéricas: más allá de la media\n",
        "\n",
        "### Medidas de tendencia central revisitadas"
      ],
      "id": "b5102c59-7dba-46ea-b2e2-ec948cfbead7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "class-output": "sourceCode"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANÁLISIS DE SALEPRICE (VARIABLE OBJETIVO) ===\n",
            "Media: $180,921.20\n",
            "Mediana: $163,000.00\n",
            "Moda: $140,000.00\n",
            "\n",
            "¿Qué nos dicen estas diferencias?\n",
            "Media > Mediana -> Distribución ASIMÉTRICA A LA DERECHA\n",
            "Interpretación: Hay viviendas muy caras que suben la media"
          ]
        }
      ],
      "source": [
        "print(\"=== ANÁLISIS DE SALEPRICE (VARIABLE OBJETIVO) ===\")\n",
        "\n",
        "# Las tres medidas de tendencia central\n",
        "media = df['SalePrice'].mean()\n",
        "mediana = df['SalePrice'].median()\n",
        "moda = df['SalePrice'].mode()[0]\n",
        "\n",
        "print(f\"Media: ${media:,.2f}\")\n",
        "print(f\"Mediana: ${mediana:,.2f}\")\n",
        "print(f\"Moda: ${moda:,.2f}\")\n",
        "\n",
        "print(\"\\n¿Qué nos dicen estas diferencias?\")\n",
        "if media > mediana:\n",
        "    print(\"Media > Mediana -> Distribución ASIMÉTRICA A LA DERECHA\")\n",
        "    print(\"Interpretación: Hay viviendas muy caras que suben la media\")\n",
        "elif media < mediana:\n",
        "    print(\"Media < Mediana -> Distribución ASIMÉTRICA A LA IZQUIERDA\")\n",
        "else:\n",
        "    print(\"Media = Mediana -> Distribución SIMÉTRICA\")"
      ],
      "id": "4bc7f896"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** La media (\\$180.921) supera a la mediana\n",
        "(\\$163.000) en \\$17.921 (un 11,0 %), señal clara de asimetría positiva:\n",
        "existe una cola de viviendas de lujo que eleva el promedio. La moda en\n",
        "\\$140.000 confirma que la mayoría de las transacciones se sitúan por\n",
        "debajo de la media. En un contexto de negocio inmobiliario, esto implica\n",
        "que la mediana es el indicador de precio más representativo del mercado\n",
        "típico, y que cualquier modelo entrenado con la media como referencia\n",
        "tendería a sobrestimar precios en el segmento mayoritario.\n",
        "\n",
        "**Recuerda (UT5):** Los *outliers* distorsionan la media de forma\n",
        "significativa pero apenas afectan a la mediana — por eso la mediana es\n",
        "la medida de centro recomendada en distribuciones asimétricas o con\n",
        "valores extremos. Tienes la explicación detallada y el ejemplo numérico\n",
        "en la **UT5 (Sección 3.2: Detección de outliers)**.\n",
        "\n",
        "### Medidas de dispersión profundas\n",
        "\n",
        "| Medida                             | Qué mide                                            | Cuándo usarla                                                 |\n",
        "|:-----------------|:---------------------|:-------------------------------|\n",
        "| **Rango**                          | Amplitud total de los datos                         | Visión rápida, muy sensible a *outliers*                      |\n",
        "| **Varianza**                       | Dispersión promedio al cuadrado respecto a la media | Base para otros cálculos, difícil de interpretar directamente |\n",
        "| **Desviación estándar**            | Dispersión promedio en unidades originales          | La más usada: mismas unidades que los datos                   |\n",
        "| **IQR**                            | Amplitud del 50% central de los datos               | Robusta ante *outliers*; ya la usaste en UT5 para detectarlos |\n",
        "| **Coeficiente de Variación (CV)**  | Dispersión relativa como porcentaje de la media     | Comparar variabilidad entre variables con escalas distintas   |\n",
        "\n",
        "Medidas de dispersión. Concepto nuevo en esta UT."
      ],
      "id": "12743542-e6c5-46d9-ab6f-5405a0a36235"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MEDIDAS DE DISPERSIÓN ===\n",
            "1. RANGO:\n",
            "   Mínimo: $34,900\n",
            "   Máximo: $755,000\n",
            "   Rango: $720,100\n",
            "\n",
            "2. VARIANZA Y DESVIACIÓN ESTÁNDAR:\n",
            "   Varianza: 6,311,111,264\n",
            "   Desviación estándar: $79,443\n",
            "   Interpretación: En promedio, los precios se desvían $79,443 de la media\n",
            "\n",
            "3. COEFICIENTE DE VARIACIÓN (CV):\n",
            "   CV: 43.91%\n",
            "   -> Alta variabilidad (datos heterogéneos)\n",
            "\n",
            "4. RANGO INTERCUARTÍLICO (IQR):\n",
            "   Q1 (25%): $129,975\n",
            "   Q3 (75%): $214,000\n",
            "   IQR: $84,025\n",
            "   Interpretación: El 50% central de viviendas está en un rango de $84,025"
          ]
        }
      ],
      "source": [
        "print(\"=== MEDIDAS DE DISPERSIÓN ===\")\n",
        "\n",
        "# Análisis completo de dispersión\n",
        "precio = df['SalePrice']\n",
        "\n",
        "print(\"1. RANGO:\")\n",
        "rango = precio.max() - precio.min()\n",
        "print(f\"   Mínimo: ${precio.min():,.0f}\")\n",
        "print(f\"   Máximo: ${precio.max():,.0f}\")\n",
        "print(f\"   Rango: ${rango:,.0f}\")\n",
        "\n",
        "print(\"\\n2. VARIANZA Y DESVIACIÓN ESTÁNDAR:\")\n",
        "varianza = precio.var()\n",
        "std = precio.std()\n",
        "print(f\"   Varianza: {varianza:,.0f}\")\n",
        "print(f\"   Desviación estándar: ${std:,.0f}\")\n",
        "print(f\"   Interpretación: En promedio, los precios se desvían ${std:,.0f} de la media\")\n",
        "\n",
        "print(\"\\n3. COEFICIENTE DE VARIACIÓN (CV):\")\n",
        "cv = (std / precio.mean()) * 100\n",
        "print(f\"   CV: {cv:.2f}%\")\n",
        "if cv < 15:\n",
        "    print(\"   -> Baja variabilidad (datos homogéneos)\")\n",
        "elif cv < 30:\n",
        "    print(\"   -> Variabilidad moderada\")\n",
        "else:\n",
        "    print(\"   -> Alta variabilidad (datos heterogéneos)\")\n",
        "\n",
        "print(\"\\n4. RANGO INTERCUARTÍLICO (IQR):\")\n",
        "# Como vimos en la UT5, el IQR es una medida de dispersión robusta que no se deja\n",
        "# distorsionar por valores extremos.\n",
        "q1 = precio.quantile(0.25)\n",
        "q3 = precio.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "print(f\"   Q1 (25%): ${q1:,.0f}\")\n",
        "print(f\"   Q3 (75%): ${q3:,.0f}\")\n",
        "print(f\"   IQR: ${iqr:,.0f}\")\n",
        "print(f\"   Interpretación: El 50% central de viviendas está en un rango de ${iqr:,.0f}\")"
      ],
      "id": "7d8a655a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** El rango bruto (\\$720.100, desde\n",
        "\\$34.900 hasta \\$755.000) confirma la presencia de viviendas muy\n",
        "atípicas que distorsionarán cualquier análisis basado en la media. La\n",
        "desviación estándar de \\$79.443 y el CV del 43,91 % indican alta\n",
        "heterogeneidad: los precios no son predecibles por la media. El IQR de\n",
        "\\$84.025 (entre \\$129.975 y \\$214.000) es la medida más fiable: el 50 %\n",
        "central de las viviendas del *dataset* oscila dentro de esa banda, que\n",
        "representa el mercado residencial estándar de Ames. Los valores fuera de\n",
        "ese rango merecen atención especial en el análisis de *outliers*.\n",
        "\n",
        "El IQR es una medida de dispersión robusta que no se deja distorsionar\n",
        "por valores extremos. En UT5 lo usamos para *detectar* outliers; aquí lo\n",
        "usamos para *cuantificar* la variabilidad central del dataset."
      ],
      "id": "8631626c-17ab-4125-aee4-84e91a53d234"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EJEMPLO: COMPARANDO VARIABILIDAD ===\n",
            "SalePrice     43.91\n",
            "GrLivArea     34.67\n",
            "YearBuilt      1.53\n",
            "OverallQual   22.67\n",
            "\n",
            "Variable más heterogénea: SalePrice"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== EJEMPLO: COMPARANDO VARIABILIDAD ===\")\n",
        "\n",
        "# Calcular CV para diferentes variables\n",
        "variables_numericas = ['SalePrice', 'GrLivArea', 'YearBuilt', 'OverallQual']\n",
        "\n",
        "cv_comparacion = (\n",
        "    df[variables_numericas]\n",
        "    .agg(lambda col: col.std() / col.mean() * 100)\n",
        "    .rename(\"CV (%)\")\n",
        ")\n",
        "print(cv_comparacion.to_string(float_format=\"{:.2f}\".format))\n",
        "\n",
        "# Identificar la más variable\n",
        "var_mas_variable = cv_comparacion.idxmax()\n",
        "print(f\"\\nVariable más heterogénea: {var_mas_variable}\")"
      ],
      "id": "b8381473"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** El CV revela diferencias dramáticas\n",
        "entre variables: `SalePrice` (43,91 %) y `GrLivArea` (34,67 %) son\n",
        "altamente heterogéneas — el precio y el área habitable varían mucho\n",
        "entre viviendas —, mientras que `YearBuilt` (1,53 %) es casi constante\n",
        "en comparación, pues todas las casas se construyeron en rangos\n",
        "temporales similares. `OverallQual` (22,67 %) muestra variabilidad\n",
        "moderada. Esta comparación justifica por qué el CV es imprescindible:\n",
        "sin él, compararíamos desviaciones en dólares, metros cuadrados y años,\n",
        "magnitudes incomparables entre sí.\n",
        "\n",
        "**Manos a la Obra**\n",
        "\n",
        "#### Comparación de medidas de dispersión\n",
        "\n",
        "**Objetivo:** Entender profundamente cuándo y por qué usar cada medida\n",
        "de dispersión analizando variables con diferentes escalas y unidades.\n",
        "\n",
        "**Contexto profesional:** Como analista de riesgos en una consultora\n",
        "inmobiliaria, debes reportar qué características de las viviendas\n",
        "presentan mayor volatilidad.\n",
        "\n",
        "**Instrucciones:**\n",
        "\n",
        "1.  Analiza estas tres variables: `LotArea` (área del terreno),\n",
        "    `YearBuilt` (año de construcción) y `GarageArea` (área del garaje).\n",
        "2.  Para cada una, calcula: Desviación estándar, Rango Intercuartílico\n",
        "    (IQR) y Coeficiente de Variación (CV).\n",
        "3.  **Análisis de datos:** Compara el CV de `LotArea` frente al de\n",
        "    `GarageArea`. ¿Cuál de las dos características es más “impredecible”\n",
        "    en relación a su magnitud media? Demuestra tu conclusión con los\n",
        "    números obtenidos.\n",
        "4.  Explica por qué el CV es la única medida justa para comparar la\n",
        "    dispersión de `YearBuilt` (escala temporal) con `LotArea` (escala\n",
        "    espacial).\n",
        "\n",
        "**Criterio de éxito:** El código debe mostrar una tabla comparativa con\n",
        "las 3 métricas para las 3 variables. La respuesta a la pregunta 3 debe\n",
        "estar fundamentada en el cálculo del CV.\n",
        "\n",
        "**Tiempo estimado:** 15 minutos\n",
        "\n",
        "``` python\n",
        "# Escribe tu codigo aqui\n",
        "```\n",
        "\n",
        "**Advertencia:** El CV solo tiene sentido para variables con **cero\n",
        "absoluto** (donde cero = ausencia total). No lo uses cuando el cero es\n",
        "arbitrario.\n",
        "\n",
        "-   **Válido:** peso (0 kg = sin peso), altura, ingresos (0€ = sin\n",
        "    ingresos)\n",
        "-   **No válido:** temperatura Celsius (0°C es el punto de congelación,\n",
        "    no “sin temperatura”), años\n",
        "\n",
        "### Forma de la distribución: asimetría y curtosis\n",
        "\n",
        "| Tipo                                | Valor Skewness                 | Forma                     | Interpretación                                                       |\n",
        "|----------|-------------------------|------------|-------------------------|\n",
        "| **Simétrica**                       | $\\approx$ 0 (entre -0.5 y 0.5) | Media $\\approx$ Mediana   | Datos equilibrados a ambos lados                                     |\n",
        "| **Asimétrica positiva** (derecha)   | \\> 0.5                         | Cola larga a la derecha   | Mayoría de valores bajos, algunos muy altos (ej: salarios, precios)  |\n",
        "| **Asimétrica negativa** (izquierda) | \\< -0.5                        | Cola larga a la izquierda | Mayoría de valores altos, algunos muy bajos (ej: edad de jubilación) |\n",
        "\n",
        "Asimetría/Skewness\n",
        "\n",
        "| Tipo             | Valor Kurtosis             | Forma                    | Interpretación                                    |\n",
        "|----------|-------------------------|------------|-------------------------|\n",
        "| **Mesocúrtica**  | $\\approx$ 0 (entre -1 y 1) | Similar a la normal      | Comportamiento «típico»                           |\n",
        "| **Leptocúrtica** | \\> 1                       | Pico alto, colas pesadas | Más concentración en el centro + más outliers     |\n",
        "| **Platicúrtica** | \\< -1                      | Más plana, colas ligeras | Datos más uniformemente dispersos, menos outliers |\n",
        "\n",
        "Curtosis\n",
        "\n",
        "En Pandas, `.kurt()` devuelve la curtosis relativa a la normal: valor 0\n",
        "= comportamiento normal, positivo = colas más pesadas, negativo = colas\n",
        "más ligeras."
      ],
      "id": "f514513a-c4c4-4242-8ff2-55d286f9282a"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANÁLISIS DE FORMA DE LA DISTRIBUCIÓN ===\n",
            "ASIMETRÍA (Skewness): 1.88\n",
            "-> Distribución ASIMÉTRICA A LA DERECHA (cola derecha larga)\n",
            "   Interpretación: Mayoría de valores bajos, algunos muy altos\n",
            "\n",
            "CURTOSIS (Kurtosis): 6.54\n",
            "-> LEPTOCÚRTICA (pico alto, colas pesadas)\n",
            "   Interpretación: Más datos concentrados en el centro + más *outliers*"
          ]
        }
      ],
      "source": [
        "print(\"=== ANÁLISIS DE FORMA DE LA DISTRIBUCIÓN ===\")\n",
        "\n",
        "precio = df['SalePrice']\n",
        "\n",
        "# Asimetría (Skewness)\n",
        "skewness = precio.skew()\n",
        "print(f\"ASIMETRÍA (Skewness): {skewness:.2f}\")\n",
        "\n",
        "if abs(skewness) < 0.5:\n",
        "    print(\"-> Distribución APROXIMADAMENTE SIMÉTRICA\")\n",
        "elif skewness > 0.5:\n",
        "    print(\"-> Distribución ASIMÉTRICA A LA DERECHA (cola derecha larga)\")\n",
        "    print(\"   Interpretación: Mayoría de valores bajos, algunos muy altos\")\n",
        "else:\n",
        "    print(\"-> Distribución ASIMÉTRICA A LA IZQUIERDA (cola izquierda larga)\")\n",
        "    print(\"   Interpretación: Mayoría de valores altos, algunos muy bajos\")\n",
        "\n",
        "# Curtosis (Kurtosis)\n",
        "kurtosis = precio.kurt()\n",
        "print(f\"\\nCURTOSIS (Kurtosis): {kurtosis:.2f}\")\n",
        "\n",
        "if abs(kurtosis) < 1:\n",
        "    print(\"-> MESOCÚRTICA (similar a normal)\")\n",
        "elif kurtosis > 1:\n",
        "    print(\"-> LEPTOCÚRTICA (pico alto, colas pesadas)\")\n",
        "    print(\"   Interpretación: Más datos concentrados en el centro + más *outliers*\")\n",
        "else:\n",
        "    print(\"-> PLATICÚRTICA (distribución más plana)\")\n",
        "    print(\"   Interpretación: Datos más dispersos uniformemente\")"
      ],
      "id": "de3d064e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** Con una asimetría (*skewness*) de 1,883,\n",
        "`SalePrice` presenta asimetría positiva fuerte: la cola de viviendas\n",
        "caras se extiende hacia la derecha, alejando la media de la mediana. La\n",
        "curtosis de 6,536 (leptocúrtica) indica colas más pesadas de lo normal,\n",
        "es decir, hay más *outliers* extremos de los esperados en una\n",
        "distribución gaussiana. Ambos valores superan los umbrales críticos\n",
        "(\\|skew\\| \\> 1 y kurtosis \\> 3), lo que confirma que una transformación\n",
        "logarítmica es necesaria antes de aplicar modelos que asumen normalidad\n",
        "como la regresión lineal. Tras aplicar $\\log$, el *skewness* cae a 0,121\n",
        "y la curtosis a 0,810 — prácticamente normales."
      ],
      "id": "4b39870b-bc65-4a81-a588-b9b30caa42f2"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== IMPACTO DE LA TRANSFORMACIÓN ===\")\n",
        "\n",
        "# Comparar antes y después de log\n",
        "precio_original = df['SalePrice']\n",
        "precio_log = np.log(df['SalePrice'])\n",
        "\n",
        "print(\"ANTES de transformación:\")\n",
        "print(f\"Skewness: {precio_original.skew():.2f}\")\n",
        "\n",
        "print(\"\\nDESPUÉS de log:\")\n",
        "print(f\"Skewness: {precio_log.skew():.2f}\")\n",
        "\n",
        "if abs(precio_log.skew()) < abs(precio_original.skew()):\n",
        "    print(\"La transformación logarítmica REDUJO la asimetría\")\n",
        "    print(\"-> Variable más adecuada para modelos que asumen normalidad\")"
      ],
      "id": "8af28006"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En proyectos de ML, una regla práctica es aplicar transformación\n",
        "logarítmica si la asimetría es mayor que 1 o menor que -1.\n",
        "\n",
        "**Manos a la Obra**\n",
        "\n",
        "#### Transformaciones y su impacto\n",
        "\n",
        "**Objetivo:** comprender cuándo y por qué transformar variables\n",
        "asimétricas.\n",
        "\n",
        "**Contexto:** estás preparando datos para un modelo de regresión lineal\n",
        "que asume normalidad. Debes identificar qué variables requieren\n",
        "transformación y justificar tu decisión.\n",
        "\n",
        "Analiza la forma de estas variables: `SalePrice`, `GrLivArea` y\n",
        "`LotArea`.\n",
        "\n",
        "1.  Para cada una: calcula *skewness* y *kurtosis*, e identifica si es\n",
        "    simétrica o asimétrica.\n",
        "2.  Si es asimétrica (`|skew| > 0.5`), aplica transformación logarítmica\n",
        "    y compara *skewness* antes/después. Verifica visualmente la mejora.\n",
        "3.  ¿Por qué la transformación logarítmica reduce la asimetría en\n",
        "    variables con distribución «lognormal»? Explica el mecanismo\n",
        "    matemático de forma intuitiva.\n",
        "4.  Comparaste *SalePrice* antes y después de log. Si el *skewness* pasó\n",
        "    de 1.8 a 0.3, ¿qué significa esto en términos prácticos para un\n",
        "    modelo de regresión lineal?\n",
        "5.  ¿Qué pasaría si aplicas log a una variable que ya es simétrica?\n",
        "    Demuéstralo calculando *skewness* antes/después en *OverallQual*.\n",
        "6.  Si una variable tiene valores de 0, no puedes aplicar log\n",
        "    directamente. ¿Qué estrategias alternativas usarías?\n",
        "7.  En un proyecto real, transformaste *SalePrice* para entrenar el\n",
        "    modelo. ¿Cómo interpretarías las predicciones del modelo? ¿Qué\n",
        "    transformación INVERSA aplicarías?\n",
        "\n",
        "**Criterio de éxito:**\n",
        "\n",
        "-   Has identificado correctamente qué variables son asimétricas.\n",
        "-   Has aplicado la transformación solo donde es necesaria.\n",
        "-   Puedes explicar el impacto de la transformación en números y\n",
        "    palabras.\n",
        "\n",
        "Consulta el **Apéndice C.5** para ver cómo calcular la asimetría y\n",
        "curtosis en Pandas.\n",
        "\n",
        "``` python\n",
        "# Escribe aquí tu código\n",
        "```\n",
        "\n",
        "**Tiempo estimado:** 15 minutos\n",
        "\n",
        "No transformes automáticamente todas las variables asimétricas. Pregunta\n",
        "primero: ¿mi modelo requiere normalidad? Árboles de decisión y modelos\n",
        "no paramétricos no necesitan transformaciones.\n",
        "\n",
        "### Implementación completa: función de análisis univariante"
      ],
      "id": "2bfef413-5767-49c2-9711-0eef61670db7"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_variable_numerica(df, variable, percentiles=[25, 50, 75]):\n",
        "    \"\"\"\n",
        "    Análisis univariante completo de una variable numérica.\n",
        "\n",
        "    Parámetros:\n",
        "    df: DataFrame de Pandas\n",
        "    variable: nombre de la columna (str)\n",
        "    percentiles: lista de percentiles a mostrar (default Q1, Q2, Q3)\n",
        "\n",
        "    Retorna:\n",
        "    dict con todas las métricas calculadas\n",
        "    \"\"\"\n",
        "    serie = df[variable]\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANÁLISIS DE: {variable}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # TENDENCIA CENTRAL\n",
        "    print(\"\\n1. TENDENCIA CENTRAL:\")\n",
        "    media = serie.mean()\n",
        "    mediana = serie.median()\n",
        "    moda = serie.mode()[0] if len(serie.mode()) > 0 else None\n",
        "\n",
        "    print(f\"   Media: {media:.2f}\")\n",
        "    print(f\"   Mediana: {mediana:.2f}\")\n",
        "    print(f\"   Moda: {moda:.2f}\" if moda else \"   Moda: No definida\")\n",
        "\n",
        "    # DISPERSIÓN\n",
        "    print(\"\\n2. DISPERSIÓN:\")\n",
        "    std = serie.std()\n",
        "    cv = (std / media) * 100 if media != 0 else None\n",
        "    iqr = serie.quantile(0.75) - serie.quantile(0.25)\n",
        "\n",
        "    print(f\"   Desv. Estándar: {std:.2f}\")\n",
        "    print(f\"   Coef. Variación: {cv:.2f}%\" if cv else \"   CV: No calculable\")\n",
        "    print(f\"   IQR: {iqr:.2f}\")\n",
        "\n",
        "    # FORMA\n",
        "    print(\"\\n3. FORMA:\")\n",
        "    skew = serie.skew()\n",
        "    kurt = serie.kurt()\n",
        "\n",
        "    print(f\"   Skewness: {skew:.2f}\", end=\" \")\n",
        "    if abs(skew) < 0.5:\n",
        "        print(\"(Simétrica)\")\n",
        "    elif skew > 0:\n",
        "        print(\"(Asimétrica derecha)\")\n",
        "    else:\n",
        "        print(\"(Asimétrica izquierda)\")\n",
        "\n",
        "    print(f\"   Kurtosis: {kurt:.2f}\", end=\" \")\n",
        "    if abs(kurt) < 1:\n",
        "        print(\"(Normal)\")\n",
        "    elif kurt > 1:\n",
        "        print(\"(Leptocúrtica - colas pesadas)\")\n",
        "    else:\n",
        "        print(\"(Platicúrtica - plana)\")\n",
        "\n",
        "    # VALORES EXTREMOS\n",
        "    print(\"\\n4. VALORES EXTREMOS:\")\n",
        "    print(f\"   Min: {serie.min():.2f}\")\n",
        "    print(f\"   Max: {serie.max():.2f}\")\n",
        "    print(f\"   Rango: {serie.max() - serie.min():.2f}\")\n",
        "\n",
        "    # PERCENTILES\n",
        "    print(f\"\\n5. PERCENTILES:\")\n",
        "    for p in percentiles:\n",
        "        print(f\"   P{p}: {serie.quantile(p/100):.2f}\")\n",
        "\n",
        "    # RECOMENDACIÓN\n",
        "    print(\"\\n6. RECOMENDACIÓN:\")\n",
        "    if abs(skew) > 1:\n",
        "        print(\"   - Variable muy asimétrica -> Considerar transformación log\")\n",
        "    if cv and cv > 50:\n",
        "        print(\"   - Alta variabilidad -> Analizar *outliers*\")\n",
        "    if kurt > 3:\n",
        "        print(\"   - Colas pesadas -> Revisar valores extremos\")\n",
        "\n",
        "    # Retornar diccionario con métricas\n",
        "    return {\n",
        "        'media': media,\n",
        "        'mediana': mediana,\n",
        "        'std': std,\n",
        "        'cv': cv,\n",
        "        'skew': skew,\n",
        "        'kurt': kurt,\n",
        "        'min': serie.min(),\n",
        "        'max': serie.max()\n",
        "    }\n",
        "\n",
        "# EJEMPLO DE USO:\n",
        "metricas_precio = analizar_variable_numerica(df, 'SalePrice')\n",
        "metricas_area = analizar_variable_numerica(df, 'GrLivArea')"
      ],
      "id": "4b20c885"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Crea una biblioteca personal de funciones como esta. ¡Te ahorrarán horas\n",
        "en futuros proyectos!\n",
        "\n",
        "### Variables categóricas: frecuencias y proporciones\n",
        "\n",
        "### Tablas de Frecuencia Enriquecidas"
      ],
      "id": "270363fc-616d-4caa-b3f5-45f444d2e5b6"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== ANÁLISIS DE VARIABLE CATEGÓRICA: NEIGHBORHOOD ===\")\n",
        "\n",
        "# Tabla de frecuencias completa\n",
        "frecuencias = df['Neighborhood'].value_counts()\n",
        "proporciones = df['Neighborhood'].value_counts(normalize=True)\n",
        "\n",
        "# Combinar en un DataFrame informativo\n",
        "tabla = pd.DataFrame({\n",
        "    'Frecuencia': frecuencias,\n",
        "    'Proporción': proporciones,\n",
        "    'Porcentaje': proporciones * 100\n",
        "})\n",
        "\n",
        "tabla['Acumulado %'] = tabla['Porcentaje'].cumsum()\n",
        "\n",
        "print(tabla.head(10))\n",
        "\n",
        "print(f\"\\nTotal de categorías: {len(frecuencias)}\")\n",
        "print(f\"Categoría más frecuente: {frecuencias.index[0]} ({frecuencias.iloc[0]} casos)\")\n",
        "print(f\"Categoría menos frecuente: {frecuencias.index[-1]} ({frecuencias.iloc[-1]} casos)\")"
      ],
      "id": "fc2c1478"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concentración de Categorías"
      ],
      "id": "52efd86a-c190-4a26-9c42-e21a4d8ca960"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== ANÁLISIS DE CONCENTRACIÓN ===\")\n",
        "\n",
        "# ¿Cuántas categorías concentran el 80% de los datos?\n",
        "acumulado = frecuencias / frecuencias.sum()\n",
        "acumulado_cum = acumulado.cumsum()\n",
        "\n",
        "n_categorias_80pct = (acumulado_cum <= 0.80).sum()\n",
        "\n",
        "print(f\"Las top {n_categorias_80pct} categorías concentran el 80% de los datos\")\n",
        "print(f\"De un total de {len(frecuencias)} categorías\")\n",
        "\n",
        "pct_concentracion = (n_categorias_80pct / len(frecuencias)) * 100\n",
        "print(f\"Nivel de concentración: {pct_concentracion:.1f}%\")\n",
        "\n",
        "if pct_concentracion < 20:\n",
        "    print(\"-> Altamente concentrado en pocas categorías\")\n",
        "elif pct_concentracion < 50:\n",
        "    print(\"-> Moderadamente concentrado\")\n",
        "else:\n",
        "    print(\"-> Distribuido uniformemente\")"
      ],
      "id": "4f25e649"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** Si el porcentaje es bajo (menos del\n",
        "20%), pocas categorías acaparan casi todos los datos. Es la señal para\n",
        "agrupar las minoritarias bajo una etiqueta “Otros” antes de modelar:\n",
        "menos ruido, modelos más estables.\n",
        "\n",
        "**Manos a la Obra**\n",
        "\n",
        "#### Análisis de concentración categórica\n",
        "\n",
        "**Objetivo:** Identificar categorías significativas y decidir\n",
        "estrategias de agrupación basadas en la frecuencia y la\n",
        "representatividad.\n",
        "\n",
        "**Contexto profesional:** Debes optimizar el pre-procesamiento de\n",
        "variables categóricas para evitar el “ruido” estadístico que generan las\n",
        "clases con muy pocos ejemplos.\n",
        "\n",
        "**Instrucciones (Semi-guiado):**\n",
        "\n",
        "1.  Analiza las variables `BldgType` (tipo de edificio) y `Condition1`\n",
        "    (proximidad a vías principales).\n",
        "2.  Para ambas, calcula la frecuencia absoluta y el porcentaje que\n",
        "    representa cada categoría sobre el total.\n",
        "3.  Identifica cuántas categorías concentran el **80% de los datos**\n",
        "    (análisis de Pareto).\n",
        "4.  Localiza aquellas categorías que aparezcan en **menos del 1%** del\n",
        "    dataset.\n",
        "5.  **Análisis por datos:** Calcula el precio medio (`SalePrice`) para\n",
        "    las categorías mayoritarias frente a las minoritarias de `BldgType`.\n",
        "    ¿Existe una diferencia significativa o las clases raras solo añaden\n",
        "    complejidad?\n",
        "\n",
        "**Pistas:**\n",
        "\n",
        "-   Usa `.value_counts(normalize=True)` para obtener proporciones.\n",
        "-   Usa `.groupby().agg()` para cruzar con el precio de venta.\n",
        "\n",
        "**Criterio de éxito:** El código debe identificar las categorías que\n",
        "concentran el 80% de la muestra y listar aquellas que deberían ser\n",
        "agrupadas como ‘Otros’ por su baja frecuencia.\n",
        "\n",
        "**Tiempo estimado:** 20 minutos\n",
        "\n",
        "``` python\n",
        "# Escribe tu codigo aqui\n",
        "```\n",
        "\n",
        "No agrupes categorías mecánicamente por frecuencia. Primero verifica si\n",
        "tienen comportamientos similares respecto al target (precio). Agrupa\n",
        "solo categorías que sean conceptualmente similares Y tengan valores de\n",
        "target comparables.\n",
        "\n",
        "### Implementación completa: función de análisis categórico"
      ],
      "id": "575e8b58-de41-4597-8586-b493138fc9ca"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analizar_variable_categorica(df, variable, top_n=10):\n",
        "    \"\"\"\n",
        "    Análisis univariante completo de una variable categórica.\n",
        "\n",
        "    Parámetros:\n",
        "    df: DataFrame\n",
        "    variable: nombre de la columna categórica\n",
        "    top_n: número de categorías principales a mostrar detalladas\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame con frecuencias y métricas\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANÁLISIS DE: {variable}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Frecuencias\n",
        "    frecuencias = df[variable].value_counts()\n",
        "    n_categorias = len(frecuencias)\n",
        "\n",
        "    print(f\"\\n1. INFORMACIÓN GENERAL:\")\n",
        "    print(f\"   Total de categorías: {n_categorias}\")\n",
        "    print(f\"   Total de valores: {df[variable].notna().sum()}\")\n",
        "    print(f\"   Valores nulos: {df[variable].isna().sum()}\")\n",
        "\n",
        "    # Tabla de frecuencias\n",
        "    tabla = pd.DataFrame({\n",
        "        'Frecuencia': frecuencias,\n",
        "        'Porcentaje': (frecuencias / len(df)) * 100\n",
        "    })\n",
        "    tabla['Acumulado %'] = tabla['Porcentaje'].cumsum()\n",
        "\n",
        "    print(f\"\\n2. TOP {top_n} CATEGORÍAS:\")\n",
        "    print(tabla.head(top_n).to_string())\n",
        "\n",
        "    # Concentración\n",
        "    top_80 = (tabla['Acumulado %'] <= 80).sum()\n",
        "    print(f\"\\n3. CONCENTRACIÓN:\")\n",
        "    print(f\"   Top {top_80} categorías = 80% de datos\")\n",
        "    print(f\"   Índice de concentración: {(top_80/n_categorias)*100:.1f}%\")\n",
        "\n",
        "    # Categorías raras\n",
        "    raras = tabla[tabla['Porcentaje'] < 1.0]\n",
        "    if len(raras) > 0:\n",
        "        print(f\"\\n4. CATEGORÍAS RARAS (< 1%):\")\n",
        "        print(f\"   Número de categorías raras: {len(raras)}\")\n",
        "        print(f\"   Representan: {raras['Porcentaje'].sum():.2f}% del total\")\n",
        "\n",
        "    # Recomendaciones\n",
        "    print(f\"\\n5. RECOMENDACIONES:\")\n",
        "    if n_categorias > 20:\n",
        "        print(f\"   - Muchas categorías ({n_categorias}) -> Considerar agrupación\")\n",
        "    if len(raras) > n_categorias * 0.3:\n",
        "        print(f\"   - {len(raras)} categorías raras -> Agrupar como 'Otros'\")\n",
        "    if top_80 < 5 and n_categorias > 10:\n",
        "        print(f\"   - Alta concentración -> Simplificar variable\")\n",
        "\n",
        "    return tabla\n",
        "\n",
        "# EJEMPLO DE USO:\n",
        "tabla_neighborhood = analizar_variable_categorica(df, 'Neighborhood')\n",
        "tabla_housestyle = analizar_variable_categorica(df, 'HouseStyle')"
      ],
      "id": "265f6834"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** El reporte impreso resume en segundos lo\n",
        "que sin la función llevaría varias líneas de código. Fíjate en las\n",
        "recomendaciones del final: «Considerar agrupación» o «Distribución\n",
        "equilibrada» son tus próximas tareas concretas en la fase de *feature\n",
        "engineering*.\n",
        "\n",
        "Combina esta función con visualizaciones (barplots) para comunicar\n",
        "efectivamente la distribución de categorías a stakeholders no técnicos.\n",
        "\n",
        "### Consolidación: análisis univariante\n",
        "\n",
        "## Análisis bivariante: relaciones entre variables\n",
        "\n",
        "### Numérica vs. numérica: correlaciones y patrones\n",
        "\n",
        "### Correlación de Pearson en Profundidad\n",
        "\n",
        "**Nomenclatura:** Este concepto se conoce por varios nombres\n",
        "equivalentes:\n",
        "\n",
        "-   **Coeficiente de correlación** (forma abreviada común)\n",
        "-   **Coeficiente de correlación de Pearson**\n",
        "-   **Coeficiente de correlación lineal de Pearson**\n",
        "-   **r de Pearson** (en notación estadística se representa como *r*)\n",
        "    $$r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$$\n",
        "\n",
        "Todos se refieren a lo mismo: una medida entre -1 y +1 que cuantifica la\n",
        "relación lineal entre dos variables. Fue desarrollado por Karl Pearson a\n",
        "finales del siglo XIX."
      ],
      "id": "ed41ddca-70b7-4f6b-a6ae-ddbba7a0732b"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== CORRELACIÓN: GRLIVAREA VS SALEPRICE ===\")\n",
        "\n",
        "# Calcular correlación\n",
        "correlacion = df['GrLivArea'].corr(df['SalePrice'])\n",
        "\n",
        "print(f\"Correlación de Pearson: {correlacion:.3f}\")\n",
        "\n",
        "# Interpretación\n",
        "print(\"\\nINTERPRETACIÓN:\")\n",
        "if abs(correlacion) < 0.3:\n",
        "    fuerza = \"DÉBIL\"\n",
        "elif abs(correlacion) < 0.7:\n",
        "    fuerza = \"MODERADA\"\n",
        "else:\n",
        "    fuerza = \"FUERTE\"\n",
        "\n",
        "direccion = \"POSITIVA\" if correlacion > 0 else \"NEGATIVA\"\n",
        "\n",
        "print(f\"Relación {fuerza} y {direccion}\")\n",
        "\n",
        "if correlacion > 0:\n",
        "    print(\"-> A mayor área habitable, mayor precio de venta\")\n",
        "else:\n",
        "    print(\"-> A mayor X, menor Y\")\n",
        "\n",
        "# Significancia práctica\n",
        "print(f\"\\nSIGNIFICANCIA PRÁCTICA:\")\n",
        "r_cuadrado = correlacion ** 2\n",
        "print(f\"R² = {r_cuadrado:.3f}\")\n",
        "print(f\"-> GrLivArea explica el {r_cuadrado*100:.1f}% de la varianza del precio\")"
      ],
      "id": "61af0cc4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ADVERTENCIA CRÍTICA — La correlación NO es causalidad**\n",
        "\n",
        "Un ejemplo clásico: las ventas de helados y los casos de ahogamiento\n",
        "tienen una correlación de r = 0.85. ¿Significa que el helado causa\n",
        "ahogamientos? **¡NO!**\n",
        "\n",
        "La explicación es que ambas variables aumentan en verano. El calor es\n",
        "una **variable confundida** (*confounding variable*) que afecta a ambas:\n",
        "más calor → más helados + más gente en piscinas.\n",
        "\n",
        "**SIEMPRE pregúntate antes de asumir causalidad:**\n",
        "\n",
        "1.  ¿Hay una explicación causal plausible?\n",
        "2.  ¿Puede haber una tercera variable que explique ambas?\n",
        "3.  ¿La correlación se mantiene en diferentes subgrupos?\n",
        "\n",
        "### Limitaciones de la Correlación de Pearson\n",
        "\n",
        "**Concepto Crítico:** La correlación de Pearson SOLO detecta relaciones\n",
        "**LINEALES**. Si la relación es curva o no lineal, Pearson puede dar\n",
        "resultados engañosos."
      ],
      "id": "df4a1e4c-4d8e-400f-af8e-9955b54c8e3f"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== LIMITACIÓN: PEARSON Y RELACIONES NO LINEALES ===\")\n",
        "\n",
        "# Generar relación cuadrática (no lineal)\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = x**2 + np.random.normal(0, 10, 100)  # Parábola con ruido\n",
        "\n",
        "# Calcular correlación\n",
        "correlacion_lineal = np.corrcoef(x, y)[0, 1]\n",
        "print(f\"Correlación de Pearson: {correlacion_lineal:.3f}\")\n",
        "print(\"-> Correlación cercana a 0 aunque hay relación clara (cuadrática)\")\n",
        "\n",
        "print(\"\\nLECCIÓN:\")\n",
        "print(\"Pearson = 0 NO significa 'no hay relación'\")\n",
        "print(\"Significa solo 'no hay relación LINEAL'\")\n",
        "print(\"\\nSOLUCIÓN: SIEMPRE visualiza con scatterplot antes de confiar en r\")"
      ],
      "id": "9400541b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlación de Spearman: la alternativa robusta"
      ],
      "id": "57b0cb06-b66b-4e78-bb9d-6aca7eee0580"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== COMPARACIÓN: PEARSON VS SPEARMAN ===\n",
            "Pearson:  0.709\n",
            "Spearman: 0.731\n",
            "Diferencia: 0.023\n",
            "\n",
            "AMBAS SIMILARES:\n",
            "-> Relación aproximadamente lineal\n",
            "-> Pearson es suficiente"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "print(\"=== COMPARACIÓN: PEARSON VS SPEARMAN ===\")\n",
        "\n",
        "# Calcular ambas\n",
        "pearson_r = df['GrLivArea'].corr(df['SalePrice'])\n",
        "spearman_r, _ = spearmanr(df['GrLivArea'], df['SalePrice'])\n",
        "\n",
        "print(f\"Pearson:  {pearson_r:.3f}\")\n",
        "print(f\"Spearman: {spearman_r:.3f}\")\n",
        "print(f\"Diferencia: {abs(pearson_r - spearman_r):.3f}\")\n",
        "\n",
        "if abs(pearson_r - spearman_r) > 0.1:\n",
        "    print(\"\\nDIFERENCIA SIGNIFICATIVA:\")\n",
        "    print(\"-> Posible relación no lineal o presencia de *outliers*\")\n",
        "    print(\"-> INVESTIGAR con scatterplot\")\n",
        "else:\n",
        "    print(\"\\nAMBAS SIMILARES:\")\n",
        "    print(\"-> Relación aproximadamente lineal\")\n",
        "    print(\"-> Pearson es suficiente\")"
      ],
      "id": "117d4a7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** Pearson da 0,709 y Spearman 0,731 para\n",
        "el par `GrLivArea`-`SalePrice`, con una diferencia de solo 0,023. Al\n",
        "estar ambas por debajo del umbral de 0,1, la relación es aproximadamente\n",
        "lineal y Pearson es suficiente en este caso. Si la diferencia hubiera\n",
        "superado 0,1, habría indicado *outliers* o curvatura que exigirían\n",
        "investigar con un diagrama de dispersión antes de modelar.\n",
        "\n",
        "**Cuándo usar cada una:**\n",
        "\n",
        "**Correlaciones y valores nulos:** `.corr()` omite los nulos en silencio\n",
        "(*pairwise deletion*): calcula cada par de variables usando solo las\n",
        "filas donde ambas tienen valor. Si el dataset tiene muchos nulos, dos\n",
        "correlaciones calculadas sobre muestras muy distintas son difícilmente\n",
        "comparables. En esta UT los datos ya vienen limpios de UT5, así que no\n",
        "hay problema — pero tenlo en cuenta cuando trabajes con datos crudos.\n",
        "\n",
        "### Matrices de correlación"
      ],
      "id": "1a572fb3-7a4f-43d0-a74d-f9f723d2260f"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MATRIZ DE CORRELACIÓN ===\n",
            "             SalePrice  GrLivArea  OverallQual  YearBuilt  TotalBsmtSF  \\\n",
            "SalePrice         1.00       0.71         0.79       0.52         0.61   \n",
            "GrLivArea         0.71       1.00         0.59       0.20         0.45   \n",
            "OverallQual       0.79       0.59         1.00       0.57         0.54   \n",
            "YearBuilt         0.52       0.20         0.57       1.00         0.39   \n",
            "TotalBsmtSF       0.61       0.45         0.54       0.39         1.00   \n",
            "GarageCars        0.64       0.47         0.60       0.54         0.43   \n",
            "\n",
            "             GarageCars  \n",
            "SalePrice          0.64  \n",
            "GrLivArea          0.47  \n",
            "OverallQual        0.60  \n",
            "YearBuilt          0.54  \n",
            "TotalBsmtSF        0.43  \n",
            "GarageCars         1.00  \n",
            "\n",
            "=== CORRELACIONES CON SALEPRICE ===\n",
            "SalePrice      1.00\n",
            "OverallQual    0.79\n",
            "GrLivArea      0.71\n",
            "GarageCars     0.64\n",
            "TotalBsmtSF    0.61\n",
            "YearBuilt      0.52\n",
            "Name: SalePrice, dtype: float64\n",
            "\n",
            "=== TOP 3 PREDICTORES ===\n",
            "                r  varianza_explicada\n",
            "OverallQual 0.791              62.600\n",
            "GrLivArea   0.709              50.200\n",
            "GarageCars  0.640              41.000\n",
            "\n",
            "=== MULTICOLINEALIDAD (Correlaciones entre predictores) ==="
          ]
        }
      ],
      "source": [
        "print(\"=== MATRIZ DE CORRELACIÓN ===\")\n",
        "\n",
        "# Seleccionar variables numéricas relevantes\n",
        "variables_interes = [\n",
        "    'SalePrice', 'GrLivArea', 'OverallQual',\n",
        "    'YearBuilt', 'TotalBsmtSF', 'GarageCars'\n",
        "]\n",
        "\n",
        "# Calcular matriz de correlación (numeric_only=True: buena práctica explícita)\n",
        "matriz_corr = df[variables_interes].corr(numeric_only=True)\n",
        "\n",
        "print(matriz_corr.round(2))\n",
        "\n",
        "# Identificar correlaciones fuertes con SalePrice\n",
        "print(\"\\n=== CORRELACIONES CON SALEPRICE ===\")\n",
        "corr_con_precio = matriz_corr['SalePrice'].sort_values(ascending=False)\n",
        "print(corr_con_precio)\n",
        "\n",
        "print(\"\\n=== TOP 3 PREDICTORES ===\")\n",
        "top3 = corr_con_precio[1:4]  # Excluir SalePrice consigo mismo\n",
        "top3_df = (\n",
        "    top3\n",
        "    .rename(\"r\")\n",
        "    .to_frame()\n",
        "    .assign(varianza_explicada=lambda df: (df[\"r\"] ** 2 * 100).round(1))\n",
        ")\n",
        "print(top3_df.to_string(float_format=\"{:.3f}\".format))\n",
        "\n",
        "# Detectar multicolinealidad\n",
        "print(\"\\n=== MULTICOLINEALIDAD (Correlaciones entre predictores) ===\")\n",
        "# Buscar pares con |r| > 0.8 (excluyendo diagonal y triángulo inferior)\n",
        "umbral = 0.8\n",
        "corr_vals = (\n",
        "    matriz_corr\n",
        "    .where(np.triu(np.ones(matriz_corr.shape, dtype=bool), k=1))\n",
        "    .stack()\n",
        ")\n",
        "pares_multicolineales = corr_vals[corr_vals.abs() > umbral]\n",
        "for (var1, var2), r in pares_multicolineales.items():\n",
        "    print(f\"{var1} <-> {var2}: r = {r:.3f}\")\n",
        "    print(f\"   -> Variables redundantes, considerar eliminar una\")"
      ],
      "id": "6090ba44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** El top predictor de `SalePrice` es\n",
        "`OverallQual` (r = 0,791), seguido de `GrLivArea` (0,709), `GarageArea`\n",
        "(0,623), `TotalBsmtSF` (0,614) y `YearBuilt` (0,523). Estas cinco\n",
        "variables explican conjuntamente gran parte de la varianza del precio.\n",
        "El análisis de multicolinealidad no detecta pares con r \\> 0,8 entre\n",
        "estos predictores, lo que indica que no hay redundancias graves en esta\n",
        "selección de variables: pueden incluirse todas en un modelo sin riesgo\n",
        "de coeficientes inestables.\n",
        "\n",
        "**Manos a la Obra**\n",
        "\n",
        "#### Análisis de correlaciones y multicolinealidad\n",
        "\n",
        "**Objetivo:** identificar las variables más predictivas y detectar\n",
        "redundancias.\n",
        "\n",
        "**Contexto:** necesitas seleccionar las mejores variables para un modelo\n",
        "de predicción de precios. Tu jefe te pidió reducir de 20 a 10 variables\n",
        "sin perder poder predictivo.\n",
        "\n",
        "Trabaja con estas 12 variables numéricas: `SalePrice`, `GrLivArea`,\n",
        "`TotalBsmtSF`, `1stFlrSF`, `GarageArea`, `GarageCars`, `TotRmsAbvGrd`,\n",
        "`OverallQual`, `YearBuilt`, `YearRemodAdd`, `MasVnrArea`, `LotArea`.\n",
        "\n",
        "1.  Calcula matriz de correlación completa (Pearson).\n",
        "2.  Identifica las 5 variables MÁS correlacionadas con `SalePrice`.\n",
        "3.  Detecta pares con multicolinealidad (`|r| > 0.75`). Para cada par\n",
        "    problemático, decide cuál eliminar.\n",
        "4.  Compara Pearson vs. Spearman en las *top* 3 variables.\n",
        "5.  Encontraste que *GarageCars* y *GarageArea* tienen *r = 0.88*. ¿Cuál\n",
        "    eliminarías y por qué?\n",
        "6.  Si *YearBuilt* y *YearRemodAdd* tienen *r = 0.65* con *SalePrice*\n",
        "    pero *r = 0.55* entre ellas, ¿las mantendrías ambas?\n",
        "7.  Supón que *TotalBsmtSF* y *1stFlrSF* tienen *r = 0.82*. ¿Qué\n",
        "    estrategia alternativa a la eliminación usarías?\n",
        "8.  Comparaste Pearson vs. Spearman para *OverallQual*. Si Spearman es\n",
        "    0.80 pero Pearson es 0.75, ¿qué te dice esa diferencia?\n",
        "9.  En producción real, si una variable tiene *r = 0.85* pero será\n",
        "    difícil de obtener para casas futuras, ¿la incluirías?\n",
        "\n",
        "**Criterio de éxito:**\n",
        "\n",
        "-   Matriz de correlación generada y legible.\n",
        "-   Has identificado correctamente multicolinealidad.\n",
        "-   Puedes justificar qué variable conservar de cada par.\n",
        "-   Has comparado Pearson vs. Spearman en al menos 3 casos.\n",
        "\n",
        "La matriz de correlación es tu mejor aliada para detectar redundancias.\n",
        "Tienes los comandos necesarios en el **Apéndice C.5**.\n",
        "\n",
        "``` python\n",
        "# Escribe aquí tu código\n",
        "```\n",
        "\n",
        "**Tiempo estimado:** 20 minutos\n",
        "\n",
        "Al detectar multicolinealidad, NO elimines automáticamente la variable\n",
        "con menor correlación individual con el *target*. Considera también:\n",
        "\n",
        "-   Facilidad de obtención del dato.\n",
        "-   Costo de recolección.\n",
        "-   Estabilidad temporal.\n",
        "-   Interpretabilidad de negocio.\n",
        "\n",
        "### Categórica vs. numérica: comparando grupos\n",
        "\n",
        "### Análisis por grupos con `groupby` avanzado"
      ],
      "id": "e640af38-dde6-45e8-a9ad-030c21ab5b48"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PRECIO PROMEDIO POR BARRIO ===\n",
            "                n   promedio   mediana        std     min     max\n",
            "Neighborhood                                                     \n",
            "NoRidge        41  335295.32  301500.0  121412.66  190000  755000\n",
            "NridgHt        77  316270.62  315000.0   96392.54  154000  611657\n",
            "StoneBr        25  310499.00  278000.0  112969.68  170000  556581\n",
            "Timber         38  242247.45  228475.0   64845.65  137500  378500\n",
            "Veenker        11  238772.73  218000.0   72369.32  162500  385000\n",
            "Somerst        86  225379.84  225500.0   56177.56  144152  423000\n",
            "ClearCr        28  212565.43  200250.0   50231.54  130000  328000\n",
            "Crawfor        51  210624.73  200624.0   68866.40   90350  392500\n",
            "CollgCr       150  197965.77  197200.0   51403.67  110000  424870\n",
            "Blmngtn        17  194870.88  191000.0   30393.23  159895  264561\n",
            "\n",
            "=== BARRIOS PREMIUM (>25% sobre promedio) ===\n",
            "               promedio  vs_promedio\n",
            "Neighborhood                        \n",
            "NoRidge       335295.32        85.33\n",
            "NridgHt       316270.62        74.81\n",
            "StoneBr       310499.00        71.62\n",
            "Timber        242247.45        33.90\n",
            "Veenker       238772.73        31.98\n",
            "\n",
            "=== BARRIOS ECONÓMICOS (<-25% bajo promedio) ===\n",
            "               promedio  vs_promedio\n",
            "Neighborhood                        \n",
            "OldTown       128225.30       -29.13\n",
            "Edwards       128219.70       -29.13\n",
            "BrkSide       124834.05       -31.00\n",
            "BrDale        104493.75       -42.24\n",
            "IDOTRR        100123.78       -44.66\n",
            "MeadowV        98576.47       -45.51"
          ]
        }
      ],
      "source": [
        "print(\"=== PRECIO PROMEDIO POR BARRIO ===\")\n",
        "\n",
        "# Análisis básico\n",
        "precio_por_barrio = df.groupby('Neighborhood')['SalePrice'].agg([\n",
        "    ('n', 'count'),\n",
        "    ('promedio', 'mean'),\n",
        "    ('mediana', 'median'),\n",
        "    ('std', 'std'),\n",
        "    ('min', 'min'),\n",
        "    ('max', 'max')\n",
        "])\n",
        "\n",
        "# Ordenar por promedio descendente\n",
        "precio_por_barrio = precio_por_barrio.sort_values('promedio', ascending=False)\n",
        "\n",
        "print(precio_por_barrio.head(10))\n",
        "\n",
        "# Identificar barrios premium\n",
        "promedio_global = df['SalePrice'].mean()\n",
        "precio_por_barrio['vs_promedio'] = \\\n",
        "    ((precio_por_barrio['promedio'] - promedio_global) / promedio_global) * 100\n",
        "\n",
        "print(\"\\n=== BARRIOS PREMIUM (>25% sobre promedio) ===\")\n",
        "premium = precio_por_barrio[precio_por_barrio['vs_promedio'] > 25]\n",
        "print(premium[['promedio', 'vs_promedio']])\n",
        "\n",
        "print(\"\\n=== BARRIOS ECONÓMICOS (<-25% bajo promedio) ===\")\n",
        "economicos = precio_por_barrio[precio_por_barrio['vs_promedio'] < -25]\n",
        "print(economicos[['promedio', 'vs_promedio']])"
      ],
      "id": "193230b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** El precio por barrio varía de forma\n",
        "dramática: NoRidge (\\$335.295), NridgHt (\\$316.271) y StoneBr\n",
        "(\\$310.499) lideran el segmento *premium*, mientras que MeadowV\n",
        "(\\$98.576) queda en el extremo opuesto. La ratio máximo/mínimo es de\n",
        "3,4x, lo que confirma que `Neighborhood` es una de las variables\n",
        "categóricas con mayor poder predictivo del *dataset*. Un modelo que no\n",
        "incluya el barrio como variable estará sistemáticamente sesgado:\n",
        "infraestimará precios en zonas *premium* y los sobreestimará en las\n",
        "económicas.\n",
        "\n",
        "No te quedes solo con la media. La desviación estándar te dice si los\n",
        "precios en ese barrio son homogéneos o muy variables. Alta *std* =\n",
        "barrio mixto (casas de diferentes calidades).\n",
        "\n",
        "### Tablas de contingencia simples"
      ],
      "id": "5ac850d7-8730-400d-b2bc-c2953b741c07"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== DISTRIBUCIÓN DE CALIDAD POR BARRIO ===\")\n",
        "\n",
        "# Tabla de contingencia\n",
        "tabla_contingencia = pd.crosstab(\n",
        "    df['Neighborhood'],\n",
        "    df['OverallQual'],\n",
        "    margins=True,\n",
        "    margins_name='Total'\n",
        ")\n",
        "\n",
        "print(tabla_contingencia.head(10))\n",
        "\n",
        "# Convertir a proporciones\n",
        "tabla_proporciones = pd.crosstab(\n",
        "    df['Neighborhood'],\n",
        "    df['OverallQual'],\n",
        "    normalize='index'  # Normalizar por fila (dentro de cada barrio)\n",
        ")\n",
        "\n",
        "print(\"\\n=== PROPORCIONES (dentro de cada barrio) ===\")\n",
        "print((tabla_proporciones * 100).round(1).head())\n",
        "\n",
        "# Identificar barrios donde >50% tienen calidad 9-10\n",
        "barrios_calidad_alta = tabla_proporciones[\n",
        "    (tabla_proporciones[9] + tabla_proporciones[10]) > 0.5\n",
        "]\n",
        "print(f\"\\nBarrios donde >50% tienen calidad 9-10:\")\n",
        "print(barrios_calidad_alta.index.tolist())"
      ],
      "id": "bff7d1d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Manos a la Obra**\n",
        "\n",
        "#### Análisis de segmentación por categorías\n",
        "\n",
        "**Objetivo:** descubrir cómo variables categóricas impactan al *target*\n",
        "y detectar segmentos de mercado.\n",
        "\n",
        "**Contexto:** eres analista de una inmobiliaria y necesitas identificar\n",
        "segmentos de mercado para una campaña de marketing dirigida. El CMO\n",
        "quiere saber: “¿qué características definen el mercado *premium*\n",
        "vs. económico?”.\n",
        "\n",
        "Realiza análisis de grupos para: `Neighborhood` (barrio), `HouseStyle`\n",
        "(estilo de casa) y `OverallQual` (calidad general).\n",
        "\n",
        "1.  Para `Neighborhood`: calcula *n*, promedio, mediana, *std*, min,\n",
        "    max. Identifica *top* 5 barrios *premium* y *bottom* 5 económicos.\n",
        "    Calcula desviación vs. promedio global.\n",
        "2.  Para `HouseStyle`: identifica estilos con mayor variabilidad de\n",
        "    precio.\n",
        "3.  Para `OverallQual`: analiza si la relación es lineal e identifica el\n",
        "    umbral donde el precio se dispara.\n",
        "4.  Crea una tabla de contingencia: `Neighborhood` vs. `OverallQual`.\n",
        "    Identifica barrios donde predominan casas de baja calidad.\n",
        "5.  Si el barrio “OldTown” tiene un precio promedio de 120 k\\$ con\n",
        "    $s = 40\\text{ k\\$}$, mientras “StoneBr” tiene 300 k\\$ con\n",
        "    $s = 80\\text{ k\\$}$, ¿cuál es MÁS homogéneo? Calcula el CV.\n",
        "6.  Encontraste que los estilos “1Story” y “2Story” tienen precios\n",
        "    promedio similares pero diferentes medianas. ¿Qué te dice esa\n",
        "    discrepancia?\n",
        "7.  Si la calidad 5 tiene un promedio de 140 k\\$ y la calidad 8 tiene\n",
        "    230 k\\$, ¿es la relación lineal? ¿Qué implicaciones tiene para\n",
        "    *feature engineering*?\n",
        "8.  En tu tabla de contingencia encontraste que el barrio “MeadowV”\n",
        "    tiene un 80 % de casas con calidad $\\leq 5$. ¿Lo mantendrías como\n",
        "    categoría separada o lo agruparías?\n",
        "9.  Imagina que tu modelo predice el precio basándose en el barrio. En\n",
        "    producción, aparece una casa en un barrio nuevo. ¿Qué estrategia\n",
        "    implementarías?\n",
        "\n",
        "**Criterio de éxito:**\n",
        "\n",
        "-   Has usado `.groupby().agg()` con múltiples funciones.\n",
        "-   Has calculado la desviación porcentual vs. promedio global.\n",
        "-   Has identificado segmentos de mercado claramente diferenciados.\n",
        "-   Has generado al menos una tabla de contingencia interpretable.\n",
        "\n",
        "El análisis por grupos con `.groupby()` es fundamental para segmentar el\n",
        "mercado. Tienes las funciones de agregación en el **Apéndice C.5**.\n",
        "\n",
        "``` python\n",
        "# Escribe aquí tu código\n",
        "```\n",
        "\n",
        "**Tiempo estimado:** 20 minutos"
      ],
      "id": "50320faf-4f81-4657-b3ee-096e3da38e3b"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ESTRUCTURA SUGERIDA (sin código completo):\n",
        "resultados = df.groupby('Variable_Categorica')['SalePrice'].agg({\n",
        "    'n': 'count',\n",
        "    'promedio': 'mean',\n",
        "    # ... añade más según necesites\n",
        "})"
      ],
      "id": "249f8e96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al presentar los resultados a *stakeholders*, ordena SIEMPRE por\n",
        "promedio descendente o ascendente. Una tabla ordenada alfabéticamente no\n",
        "comunica el *insight* de “qué barrios son *premium*”.\n",
        "\n",
        "### Categórica vs. categórica: tablas de contingencia"
      ],
      "id": "c81d801f-11eb-4402-96e1-4843f879e44a"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== RELACIÓN: GARAGETYPE VS HOUSESTYLE ===\")\n",
        "\n",
        "# Crear tabla de contingencia\n",
        "tabla = pd.crosstab(\n",
        "    df['GarageType'],\n",
        "    df['HouseStyle'],\n",
        "    margins=True\n",
        ")\n",
        "\n",
        "print(tabla)\n",
        "\n",
        "# Convertir a proporciones por fila\n",
        "tabla_prop = pd.crosstab(\n",
        "    df['GarageType'],\n",
        "    df['HouseStyle'],\n",
        "    normalize='index'\n",
        ")\n",
        "\n",
        "print(\"\\n=== PROPORCIONES (por tipo de garaje) ===\")\n",
        "print((tabla_prop * 100).round(1))\n",
        "\n",
        "# Análisis\n",
        "print(\"\\nINTERPRETACIÓN:\")\n",
        "print(\"Si hay asociación, las proporciones variarán entre filas\")\n",
        "print(\"Si son independientes, las proporciones serán similares\")"
      ],
      "id": "c9c2f2ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** Mira las proporciones por fila. Si\n",
        "cambian mucho de una fila a otra, las dos variables están asociadas: el\n",
        "tipo de garaje influye en el estilo de vivienda. Si las proporciones son\n",
        "parecidas en todas las filas, son independientes y puedes tratarlas por\n",
        "separado.\n",
        "\n",
        "### Concepto Intuitivo de Independencia"
      ],
      "id": "cafb4ae8-14a6-4b71-ae8a-519a88fa85f1"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== DETECTAR INDEPENDENCIA VISUALMENTE ===\")\n",
        "\n",
        "# Si las proporciones son MUY similares entre grupos -> Independencia\n",
        "# Si varían mucho -> Asociación\n",
        "\n",
        "tabla_prop = pd.crosstab(\n",
        "    df['CentralAir'],  # Aire acondicionado (Y/N)\n",
        "    df['Heating'],     # Tipo de calefacción\n",
        "    normalize='index'\n",
        ")\n",
        "\n",
        "print(tabla_prop * 100)\n",
        "\n",
        "# Calcular variabilidad de proporciones (vectorizado por columna)\n",
        "variabilidad = tabla_prop.std().rename(\"std_proporciones\")\n",
        "etiquetas = variabilidad.apply(\n",
        "    lambda v: \"posible independencia\" if v < 0.05 else \"hay asociación\"\n",
        ").rename(\"interpretación\")\n",
        "print(pd.concat([variabilidad.round(4), etiquetas], axis=1).to_string())"
      ],
      "id": "b280cb4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Manos a la Obra**\n",
        "\n",
        "#### Análisis de independencia entre categóricas\n",
        "\n",
        "**Objetivo:** Determinar mediante análisis de contingencia si existe una\n",
        "asociación estadística relevante entre diferentes características\n",
        "cualitativas de las viviendas.\n",
        "\n",
        "**Contexto profesional:** Como parte de un proceso de selección de\n",
        "características (*feature selection*), debes identificar variables\n",
        "categóricas redundantes (altamente asociadas) para simplificar el modelo\n",
        "final y evitar multicolinealidad cualitativa.\n",
        "\n",
        "**Tu misión (autónomo):**\n",
        "\n",
        "1.  Selecciona dos pares de variables categóricas que creas que podrían\n",
        "    estar relacionadas (ej: `GarageType` vs `GarageFinish`) y otro par\n",
        "    que creas que debería ser independiente.\n",
        "2.  Genera las tablas de contingencia normalizadas por filas para ambos\n",
        "    casos.\n",
        "3.  **Juicio profesional:** Analiza los resultados y determina,\n",
        "    basándote exclusivamente en los datos obtenidos, qué par presenta\n",
        "    una asociación real y cuál parece ser independiente. Demuestra tu\n",
        "    conclusión señalando cómo varían las proporciones en las celdas de\n",
        "    la tabla.\n",
        "4.  Justifica cuál de las variables eliminarías del modelo si\n",
        "    encontraras una asociación superior al 90%.\n",
        "\n",
        "**Criterio de éxito:** Presentar dos tablas de contingencia legibles y\n",
        "una conclusión escrita que identifique correctamente la presencia o\n",
        "ausencia de asociación basada en la variabilidad de las frecuencias\n",
        "relativas.\n",
        "\n",
        "**Tiempo estimado:** 15 minutos\n",
        "\n",
        "``` python\n",
        "# Escribe tu codigo aqui\n",
        "```\n",
        "\n",
        "Si trabajas con más de 2 variables categóricas, considera usar análisis\n",
        "de correspondencias múltiples (MCA) o chi-cuadrado de forma sistemática.\n",
        "Para 2-3 pares, el análisis visual de tablas es suficiente y más\n",
        "interpretable.\n",
        "\n",
        "### Consolidación: análisis bivariante\n",
        "\n",
        "## Análisis multivariante: explorando interacciones\n",
        "\n",
        "### Estratificación: Análisis Condicional"
      ],
      "id": "a8a818b4-c7e2-46d5-9eda-28e159ec18a2"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== CORRELACIÓN GrLivArea-SalePrice ESTRATIFICADA POR BARRIO ===\")\n",
        "\n",
        "# Seleccionar top 5 barrios por frecuencia\n",
        "top_barrios = df['Neighborhood'].value_counts().head(5).index\n",
        "\n",
        "# Calcular correlación estratificada por barrio (vectorizado con groupby+apply)\n",
        "def corr_grlivarea_saleprice(sub):\n",
        "    return sub['GrLivArea'].corr(sub['SalePrice'])\n",
        "\n",
        "df_resultados = (\n",
        "    df[df['Neighborhood'].isin(top_barrios)]\n",
        "    .groupby('Neighborhood')\n",
        "    .apply(corr_grlivarea_saleprice, include_groups=False)\n",
        "    .rename(\"Correlación\")\n",
        "    .to_frame()\n",
        "    .assign(n=df[df['Neighborhood'].isin(top_barrios)].groupby('Neighborhood').size())\n",
        "    .loc[top_barrios]\n",
        ")\n",
        "print(df_resultados.to_string(float_format=\"{:.3f}\".format))\n",
        "\n",
        "print(\"\\n=== ANÁLISIS ===\")\n",
        "print(f\"Correlación mínima: {df_resultados['Correlación'].min():.3f}\")\n",
        "print(f\"Correlación máxima: {df_resultados['Correlación'].max():.3f}\")\n",
        "print(f\"Rango: {df_resultados['Correlación'].max() - df_resultados['Correlación'].min():.3f}\")\n",
        "\n",
        "if df_resultados['Correlación'].std() > 0.15:\n",
        "    print(\"\\nHAY INTERACCIÓN:\")\n",
        "    print(\"La relación GrLivArea-SalePrice VARÍA según el barrio\")\n",
        "    print(\"-> Considera crear features de interacción: GrLivArea * Neighborhood\")\n",
        "else:\n",
        "    print(\"\\nNO HAY INTERACCIÓN:\")\n",
        "    print(\"La relación es consistente entre barrios\")"
      ],
      "id": "1d0866ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agregaciones múltiples y *pivoting*"
      ],
      "id": "6ae184d6-e8ec-4000-807f-ccae9ced77c8"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== PRECIO PROMEDIO POR BARRIO Y CALIDAD ===\")\n",
        "\n",
        "# Pivot table: Barrio (filas) x Calidad (columnas)\n",
        "pivot = df.pivot_table(\n",
        "    values='SalePrice',\n",
        "    index='Neighborhood',\n",
        "    columns='OverallQual',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "# Mostrar solo top barrios y calidades relevantes\n",
        "barrios_relevantes = df['Neighborhood'].value_counts().head(8).index\n",
        "calidades_relevantes = [5, 6, 7, 8, 9, 10]\n",
        "\n",
        "pivot_filtrado = pivot.loc[barrios_relevantes, calidades_relevantes]\n",
        "\n",
        "print(pivot_filtrado.round(0))\n",
        "\n",
        "# Análisis de patrones (vectorizado por barrio con apply)\n",
        "print(\"\\n=== PATRONES DETECTADOS ===\")\n",
        "\n",
        "def analizar_patron(serie):\n",
        "    incrementos = serie.dropna().sort_index().diff().dropna()\n",
        "    if len(incrementos) < 2:\n",
        "        return pd.Series({'incremento_promedio': float('nan'), 'std_incremento': float('nan'), 'no_lineal': False})\n",
        "    media = incrementos.mean()\n",
        "    std = incrementos.std()\n",
        "    return pd.Series({'incremento_promedio': media, 'std_incremento': std, 'no_lineal': (std / media > 0.5) if media else False})\n",
        "\n",
        "patrones = (\n",
        "    pivot.loc[barrios_relevantes]\n",
        "    .apply(analizar_patron, axis=1)\n",
        ")\n",
        "print(patrones.to_string(float_format='{:,.0f}'.format))"
      ],
      "id": "7b31534a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** Busca los incrementos irregulares: si\n",
        "subir un punto de calidad dobla el precio en un barrio pero apenas lo\n",
        "mueve en otro, la relación no es lineal. Esa variabilidad es exactamente\n",
        "la información que necesitas para crear variables de interacción en la\n",
        "fase de modelado.\n",
        "\n",
        "### *Pivot tables* avanzadas: múltiples agregaciones"
      ],
      "id": "98dc7af7-259d-4a24-8106-781c41a390aa"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== PIVOT TABLE CON MÚLTIPLES AGREGACIONES ===\")\n",
        "\n",
        "# Analizar precio por barrio y calidad con varias métricas\n",
        "pivot_completo = df.pivot_table(\n",
        "    values='SalePrice',\n",
        "    index='Neighborhood',\n",
        "    columns='OverallQual',\n",
        "    aggfunc=['mean', 'count', 'std']\n",
        ")\n",
        "\n",
        "# Acceder a diferentes agregaciones\n",
        "print(\"PROMEDIO:\")\n",
        "print(pivot_completo['mean'].head())\n",
        "\n",
        "print(\"\\nCONTEO (n por celda):\")\n",
        "print(pivot_completo['count'].head())\n",
        "\n",
        "# Identificar celdas con pocos datos (n < 5)\n",
        "conteos = pivot_completo['count']\n",
        "celdas_problematicas = conteos < 5\n",
        "\n",
        "print(f\"\\n=== CELDAS CON DATOS INSUFICIENTES (n<5) ===\")\n",
        "print(f\"Total de combinaciones: {conteos.size}\")\n",
        "print(f\"Combinaciones con n<5: {celdas_problematicas.sum().sum()}\")\n",
        "print(f\"Porcentaje problemático: {(celdas_problematicas.sum().sum() / conteos.size) * 100:.1f}%\")"
      ],
      "id": "08898ec6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Análisis de los resultados:** Fíjate en el porcentaje de celdas con\n",
        "n\\<5. Si es alto, la tabla tiene demasiadas combinaciones vacías o casi\n",
        "vacías para ser fiable. En ese caso, considera reducir la granularidad:\n",
        "agrupar barrios o calidades antes de analizar.\n",
        "\n",
        "**Manos a la Obra**\n",
        "\n",
        "#### Análisis de interacciones multivariante\n",
        "\n",
        "**Objetivo:** Detectar interacciones entre variables que no son visibles\n",
        "mediante análisis bivariante simple y cuantificar su impacto en el\n",
        "precio.\n",
        "\n",
        "**Contexto profesional:** Tu director comercial te plantea una\n",
        "hipótesis: *“Creo que los metros cuadrados no se valoran igual en un\n",
        "barrio humilde que en uno de lujo”*. Tu tarea es demostrar o desmentir\n",
        "esta hipótesis con evidencias estadísticas.\n",
        "\n",
        "**Tu misión (autónomo):**\n",
        "\n",
        "1.  Selecciona los **3 barrios con más registros** del dataset.\n",
        "2.  Calcula la correlación entre la superficie habitable (`GrLivArea`) y\n",
        "    el precio (`SalePrice`) de forma independiente para cada uno de esos\n",
        "    3 barrios.\n",
        "3.  Crea una tabla dinámica (`pivot_table`) que muestre el precio medio\n",
        "    cruzando el barrio (`Neighborhood`) con la calidad general\n",
        "    (`OverallQual`) para los barrios seleccionados.\n",
        "4.  **Informe de hallazgos:** ¿Es constante la relación entre metros\n",
        "    cuadrados y precio en los tres barrios? Identifica una combinación\n",
        "    de Barrio/Calidad donde el precio medio sea inusualmente alto o bajo\n",
        "    en comparación con la tendencia general.\n",
        "\n",
        "**Criterio de éxito:** Presentar las 3 correlaciones diferenciadas y la\n",
        "tabla dinámica. La conclusión debe responder explícitamente a la\n",
        "hipótesis del director comercial usando los datos obtenidos.\n",
        "\n",
        "**Tiempo estimado:** 25 minutos\n",
        "\n",
        "``` python\n",
        "# Escribe tu codigo aqui\n",
        "```\n",
        "\n",
        "En modelos tree-based (Random Forest, XGBoost), las interacciones se\n",
        "capturan automáticamente. En regresión lineal, debes crearlas\n",
        "manualmente. Esto afecta tu estrategia de *feature engineering*.\n",
        "\n",
        "### Consolidación: análisis multivariante\n",
        "\n",
        "## Estadística inferencial básica: de la muestra a la población\n",
        "\n",
        "### Conceptos fundamentales: población vs. muestra"
      ],
      "id": "040dd9f7-b488-4d03-a719-a8bbee042859"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== POBLACIÓN VS MUESTRA ===\")\n",
        "\n",
        "# Simular población completa (todas las casas en la ciudad)\n",
        "np.random.seed(42)\n",
        "poblacion_completa = np.random.normal(loc=180000, scale=50000, size=100000)\n",
        "\n",
        "# Tu dataset es una muestra\n",
        "muestra = df['SalePrice']\n",
        "\n",
        "print(f\"POBLACIÓN (imaginaria):\")\n",
        "print(f\"  Tamaño: {len(poblacion_completa):,} casas\")\n",
        "print(f\"  Media verdadera: ${poblacion_completa.mean():,.0f}\")\n",
        "\n",
        "print(f\"\\nMUESTRA (tu *dataset*):\")\n",
        "print(f\"  Tamaño: {len(muestra)} casas\")\n",
        "print(f\"  Media de la muestra: ${muestra.mean():,.0f}\")\n",
        "\n",
        "print(f\"\\nDIFERENCIA:\")\n",
        "print(f\"  Error muestral: ${abs(muestra.mean() - poblacion_completa.mean()):,.0f}\")\n",
        "print(f\"  Porcentaje: {abs(muestra.mean() - poblacion_completa.mean()) / poblacion_completa.mean() * 100:.2f}%\")"
      ],
      "id": "4a9cbaea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error estándar: variabilidad de las estimaciones"
      ],
      "id": "4b99eb95-a974-438f-a996-e8a901e1abed"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== ERROR ESTÁNDAR ===\")\n",
        "\n",
        "n = len(muestra)\n",
        "std_muestra = muestra.std()\n",
        "\n",
        "# Fórmula del error estándar de la media\n",
        "se = std_muestra / np.sqrt(n)\n",
        "\n",
        "print(f\"Desviación estándar de la muestra: ${std_muestra:,.0f}\")\n",
        "print(f\"Tamaño de la muestra: {n}\")\n",
        "print(f\"Error estándar: ${se:,.0f}\")\n",
        "\n",
        "print(f\"\\nINTERPRETACIÓN:\")\n",
        "print(f\"Si repitiéramos el muestreo 100 veces, las medias variarían\")\n",
        "print(f\"típicamente en +/- ${se:,.0f} alrededor de la media verdadera\")\n",
        "\n",
        "# Efecto del tamaño muestral (vectorizado con Series)\n",
        "print(f\"\\n=== EFECTO DEL TAMAÑO MUESTRAL ===\")\n",
        "n_simulados = pd.Series([10, 50, 100, 500, 1000], name='n')\n",
        "se_por_n = (std_muestra / np.sqrt(n_simulados)).rename('SE ($)').round(0).astype(int)\n",
        "print(pd.concat([n_simulados, se_por_n], axis=1).to_string(index=False))\n",
        "\n",
        "print(\"\\nCONCLUSIÓN: a mayor n, menor incertidumbre\")"
      ],
      "id": "da35c518"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intervalos de confianza: rango creíble"
      ],
      "id": "5d2ed456-478c-4294-968b-60b3ef7b4b44"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "print(\"=== INTERVALO DE CONFIANZA 95% PARA LA MEDIA ===\")\n",
        "\n",
        "# Calcular IC usando t-student (apropiado para muestras)\n",
        "confianza = 0.95\n",
        "grados_libertad = len(muestra) - 1\n",
        "t_critico = stats.t.ppf((1 + confianza) / 2, grados_libertad)\n",
        "\n",
        "media_muestra = muestra.mean()\n",
        "se = muestra.std() / np.sqrt(len(muestra))\n",
        "\n",
        "margen_error = t_critico * se\n",
        "ic_inferior = media_muestra - margen_error\n",
        "ic_superior = media_muestra + margen_error\n",
        "\n",
        "print(f\"Media de la muestra: ${media_muestra:,.0f}\")\n",
        "print(f\"Error estándar: ${se:,.0f}\")\n",
        "print(f\"Margen de error (95%): ${margen_error:,.0f}\")\n",
        "\n",
        "print(f\"\\nINTERVALO DE CONFIANZA 95%:\")\n",
        "print(f\"[${ic_inferior:,.0f}, ${ic_superior:,.0f}]\")\n",
        "\n",
        "print(f\"\\nINTERPRETACIÓN EN LENGUAJE DE NEGOCIO:\")\n",
        "print(f\"Estamos 95% confiados de que el precio promedio REAL de\")\n",
        "print(f\"todas las casas en esta ciudad está entre:\")\n",
        "print(f\"${ic_inferior:,.0f} y ${ic_superior:,.0f}\")"
      ],
      "id": "122b0e68"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test de hipótesis: la lógica del contraste\n",
        "\n",
        "### Ejemplo conceptual: ¿hay diferencia real?"
      ],
      "id": "7a6ae4c2-6e49-4bba-91fe-f71d6fc57a07"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== EJEMPLO: COMPARAR PRECIOS ENTRE DOS BARRIOS ===\")\n",
        "\n",
        "# Seleccionar dos barrios\n",
        "barrio_A = df[df['Neighborhood'] == 'CollgCr']['SalePrice']\n",
        "barrio_B = df[df['Neighborhood'] == 'OldTown']['SalePrice']\n",
        "\n",
        "media_A = barrio_A.mean()\n",
        "media_B = barrio_B.mean()\n",
        "diferencia = media_A - media_B\n",
        "\n",
        "print(f\"Barrio A (CollgCr): ${media_A:,.0f} (n={len(barrio_A)})\")\n",
        "print(f\"Barrio B (OldTown): ${media_B:,.0f} (n={len(barrio_B)})\")\n",
        "print(f\"Diferencia: ${diferencia:,.0f}\")\n",
        "\n",
        "# PREGUNTA: ¿esta diferencia es \"real\" o \"azar\"?\n",
        "\n",
        "print(\"\\n=== RAZONAMIENTO ===\")\n",
        "print(\"Si la diferencia fuera solo por azar (mala suerte en el muestreo),\")\n",
        "print(\"esperaríamos que fuera pequeña relativa a la variabilidad de los datos.\")\n",
        "\n",
        "# Calcular \"efecto estandarizado\"\n",
        "std_pooled = np.sqrt((barrio_A.std()**2 + barrio_B.std()**2) / 2)\n",
        "efecto_estandarizado = diferencia / std_pooled\n",
        "\n",
        "print(f\"\\nEfecto estandarizado (Cohen's d): {efecto_estandarizado:.2f}\")\n",
        "\n",
        "if abs(efecto_estandarizado) < 0.3:\n",
        "    print(\"-> Efecto PEQUEÑO (probablemente azar)\")\n",
        "elif abs(efecto_estandarizado) < 0.8:\n",
        "    print(\"-> Efecto MODERADO (hay algo real)\")\n",
        "else:\n",
        "    print(\"-> Efecto GRANDE (diferencia muy clara)\")"
      ],
      "id": "87e805de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretación del *p-valor*\n",
        "\n",
        "**Advertencias críticas sobre el *p-valor*:**\n",
        "\n",
        "1.  El *p-valor* **NO** es «la probabilidad de que H<sub>0</sub> sea\n",
        "    verdadera»\n",
        "2.  p \\< 0.05 **NO** es sagrado, es solo una convención histórica\n",
        "3.  Significancia estadística $\\neq$ importancia práctica\n",
        "4.  Con n suficientemente grande, TODO puede ser “estadísticamente\n",
        "    significativo”\n",
        "\n",
        "### Casos de uso en proyectos reales\n",
        "\n",
        "### Tests formales en Python: solo referencia\n",
        "\n",
        "**1. Test t de Student para dos muestras independientes:**"
      ],
      "id": "686020a2-6617-4667-8220-9056a3d47920"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Comparar medias de dos grupos\n",
        "statistic, pvalue = ttest_ind(grupo1, grupo2)\n",
        "\n",
        "# SUPUESTOS REQUERIDOS:\n",
        "# - Normalidad en cada grupo (o n>=30 por TCL)\n",
        "# - Homocedasticidad (varianzas iguales)\n",
        "# - Muestras independientes\n",
        "\n",
        "# SI NO SE CUMPLE homocedasticidad:\n",
        "statistic, pvalue = ttest_ind(grupo1, grupo2, equal_var=False)  # Test de Welch"
      ],
      "id": "bbda5d11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Test t de Student para muestras emparejadas:**"
      ],
      "id": "c657e722-6c7c-4772-9677-3554ca27bea9"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Comparar antes vs después en los mismos sujetos\n",
        "statistic, pvalue = ttest_rel(antes, despues)\n",
        "\n",
        "# SUPUESTOS REQUERIDOS:\n",
        "# - Normalidad de las DIFERENCIAS (no de cada grupo por separado)\n",
        "# - Observaciones emparejadas (mismo sujeto/unidad medido dos veces)"
      ],
      "id": "34635506"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. Test de correlación de Pearson:**"
      ],
      "id": "906412e9-9db7-4dc6-9830-4caa5305fb2c"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# ¿La correlación observada es estadísticamente significativa?\n",
        "corr, pvalue = pearsonr(x, y)\n",
        "\n",
        "# SUPUESTOS REQUERIDOS:\n",
        "# - Relación LINEAL (Pearson no detecta relaciones curvas)\n",
        "# - Normalidad bivariada (especialmente importante para n<30)\n",
        "# - Ausencia de *outliers* extremos"
      ],
      "id": "0729ee54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. Test Chi-cuadrado de independencia:**"
      ],
      "id": "8a04dbd5-7f4b-4fca-93bc-10079647c360"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# ¿Dos variables categóricas son independientes?\n",
        "tabla = pd.crosstab(df['Cat1'], df['Cat2'])\n",
        "chi2, pvalue, dof, expected = chi2_contingency(tabla)\n",
        "\n",
        "# SUPUESTOS REQUERIDOS:\n",
        "# - Frecuencias esperadas >= 5 en al menos 80% de celdas\n",
        "# - Ninguna frecuencia esperada < 1\n",
        "# - Observaciones independientes (cada caso cuenta una sola vez)\n",
        "\n",
        "# Verificar frecuencias esperadas:\n",
        "print(\"Frecuencias esperadas:\")\n",
        "print(expected)\n",
        "if (expected < 5).sum() > 0.2 * expected.size:\n",
        "    print(\" ADVERTENCIA: Muchas celdas con frecuencia esperada < 5\")\n",
        "    print(\"-> Considera agrupar categorías raras\")"
      ],
      "id": "63edd9ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**VERIFICACIÓN DE SUPUESTOS - Ejemplos:**"
      ],
      "id": "298dae96-3fc0-42db-b7c2-907d0683d1ca"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import shapiro, levene\n",
        "\n",
        "# Test de normalidad (Shapiro-Wilk)\n",
        "# H0: Los datos provienen de una distribución normal\n",
        "statistic, pvalue = shapiro(datos)\n",
        "if pvalue < 0.05:\n",
        "    print(\"Rechazamos normalidad -> Considerar test no paramétrico\")\n",
        "\n",
        "# Test de homocedasticidad (Levene)\n",
        "# H0: Las varianzas de los grupos son iguales\n",
        "statistic, pvalue = levene(grupo1, grupo2)\n",
        "if pvalue < 0.05:\n",
        "    print(\"Rechazamos homocedasticidad -> Usar test de Welch\")"
      ],
      "id": "78d4ffb1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ALTERNATIVAS NO PARAMÉTRICAS** (cuando no se cumplen supuestos):"
      ],
      "id": "f050a03b-e930-4ee7-8f16-5f7e67a5268e"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu, wilcoxon, spearmanr\n",
        "\n",
        "# Mann-Whitney U (alternativa no paramétrica a t-test independiente)\n",
        "# No requiere normalidad, solo que distribuciones tengan forma similar\n",
        "statistic, pvalue = mannwhitneyu(grupo1, grupo2)\n",
        "\n",
        "# Wilcoxon (alternativa no paramétrica a t-test emparejado)\n",
        "statistic, pvalue = wilcoxon(antes, despues)\n",
        "\n",
        "# Spearman (alternativa no paramétrica a correlación de Pearson)\n",
        "# Basada en rangos, robusta a *outliers* y relaciones no lineales\n",
        "corr, pvalue = spearmanr(x, y)"
      ],
      "id": "bacee51e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En proyectos profesionales, si tienes dudas sobre supuestos, consulta a\n",
        "un estadístico. Un análisis erróneo puede llevar a decisiones de negocio\n",
        "costosas.\n",
        "\n",
        "### ¿Cuándo necesitas tests formales?\n",
        "\n",
        "### Consolidación: estadística inferencial básica\n",
        "\n",
        "## Metodología de EDA: flujo de trabajo profesional\n",
        "\n",
        "### *Checklist* del EDA completo\n",
        "\n",
        "Usa este checklist para asegurar que no omites pasos críticos:\n",
        "\n",
        "**FASE 1: PRIMERA INSPECCIÓN (15 min)**\n",
        "\n",
        "-   [ ] Cargar datos y verificar que se cargaron correctamente\n",
        "-   [ ] Verificar dimensiones con .shape\n",
        "-   [ ] Inspeccionar primeras y últimas filas con .head() y .tail()\n",
        "-   [ ] Revisar tipos de datos con .dtypes\n",
        "-   [ ] Verificar valores nulos con .isnull().sum()\n",
        "-   [ ] Obtener resumen estadístico con .describe()\n",
        "\n",
        "**FASE 2: ANÁLISIS UNIVARIANTE (30-60 min)**\n",
        "\n",
        "Numéricas:\n",
        "\n",
        "-   [ ] Calcular tendencia central (media, mediana, moda)\n",
        "-   [ ] Calcular dispersión (std, CV, IQR)\n",
        "-   [ ] Calcular forma (skewness, kurtosis)\n",
        "-   [ ] Identificar necesidad de transformaciones\n",
        "-   [ ] Crear histogramas para variables clave\n",
        "\n",
        "Categóricas:\n",
        "\n",
        "-   [ ] Calcular frecuencias y proporciones\n",
        "-   [ ] Identificar categorías raras (\\< 1%)\n",
        "-   [ ] Analizar concentración (regla 80/20)\n",
        "-   [ ] Decidir estrategia de agrupación si necesaria\n",
        "\n",
        "**FASE 3: ANÁLISIS BIVARIANTE (45-90 min)**\n",
        "\n",
        "Numérica-Numérica:\n",
        "\n",
        "-   [ ] Calcular matriz de correlación\n",
        "-   [ ] Identificar top 5 predictores del target\n",
        "-   [ ] Detectar multicolinealidad (\\|r\\| \\> 0.75)\n",
        "-   [ ] Comparar Pearson vs Spearman en casos dudosos\n",
        "-   [ ] Crear scatterplots para relaciones fuertes\n",
        "\n",
        "Categórica-Numérica:\n",
        "\n",
        "-   [ ] Análisis por grupos (.groupby)\n",
        "-   [ ] Identificar categorías con mayor/menor target\n",
        "-   [ ] Calcular variabilidad dentro de categorías\n",
        "-   [ ] Detectar segmentos de mercado\n",
        "\n",
        "Categórica-Categórica:\n",
        "\n",
        "-   [ ] Tablas de contingencia para pares relevantes\n",
        "-   [ ] Evaluar independencia visualmente\n",
        "-   [ ] Identificar asociaciones fuertes\n",
        "\n",
        "**FASE 4: ANÁLISIS MULTIVARIANTE (60-120 min)**\n",
        "\n",
        "-   [ ] Estratificar correlaciones por subgrupos\n",
        "-   [ ] Crear pivot tables multidimensionales\n",
        "-   [ ] Detectar interacciones clave (efecto diferencial)\n",
        "-   [ ] Analizar no-linealidades y umbrales\n",
        "\n",
        "**FASE 5: DOCUMENTACIÓN (30 min)**\n",
        "\n",
        "-   [ ] Crear sección de hallazgos clave\n",
        "-   [ ] Documentar decisiones de transformación\n",
        "-   [ ] Listar variables a eliminar/agrupar\n",
        "-   [ ] Anotar hipótesis para feature engineering\n",
        "-   [ ] Preparar 2-3 visualizaciones clave para stakeholders\n",
        "\n",
        "### Registro de hallazgos clave\n",
        "\n",
        "### *Template* de *notebook* de EDA\n",
        "\n",
        "Añade celdas de Markdown entre bloques de código para explicar QUÉ estás\n",
        "buscando y POR QUÉ es relevante. Un notebook sin narrativa es solo\n",
        "código difícil de seguir.\n",
        "\n",
        "### Preparación para modelado: *checklist* final\n",
        "\n",
        "Antes de pasar a entrenar modelos (UT8), verifica:\n",
        "\n",
        "**Variables:**\n",
        "\n",
        "-   [ ] Variables finales seleccionadas documentadas\n",
        "-   [ ] Multicolinealidad resuelta (\\|r\\| \\< 0.8 entre predictores)\n",
        "-   [ ] Categorías raras agrupadas o eliminadas\n",
        "\n",
        "**Transformaciones:**\n",
        "\n",
        "-   [ ] Variables asimétricas transformadas (log, sqrt, etc.)\n",
        "-   [ ] Transformaciones documentadas (para invertir predicciones)\n",
        "-   [ ] Escalado no necesario aún (se hace en UT8)\n",
        "\n",
        "**Datos limpios:**\n",
        "\n",
        "-   [ ] Nulos manejados (de UT5)\n",
        "-   [ ] Outliers revisados y decisión tomada\n",
        "-   [ ] Duplicados eliminados (de UT5)\n",
        "\n",
        "**Split Train/Test:**\n",
        "\n",
        "-   [ ] Train/Test definido (80/20)\n",
        "-   [ ] Estratificado si hay desbalance\n",
        "-   [ ] Sin data leakage (split ANTES de imputación/scaling)\n",
        "\n",
        "**Documentación:**\n",
        "\n",
        "-   [ ] Decisiones justificadas con EDA\n",
        "-   [ ] Supuestos documentados\n",
        "-   [ ] Baseline definido\n",
        "\n",
        "### Errores comunes y cómo evitarlos\n",
        "\n",
        "## Ejemplos de referencia: patrones EDA\n",
        "\n",
        "### Análisis Univariante (Numérico y Categórico)"
      ],
      "id": "897c378e-0b8c-4b6e-921f-1168edc06f75"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Medidas de tendencia y dispersión robustas\n",
        "stats = df['col'].agg(['mean', 'median', 'std'])\n",
        "cv = df['col'].std() / df['col'].mean() # Coeficiente de Variación\n",
        "\n",
        "# 2. Concentración categórica (frecuencias)\n",
        "freq = df['cat'].value_counts()\n",
        "rel_freq = df['cat'].value_counts(normalize=True) * 100"
      ],
      "id": "ab7506aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis Bivariante y Correlaciones"
      ],
      "id": "20c0ffbd-0781-429c-aa09-8997b5940606"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Matriz de correlación de Pearson\n",
        "corr_matrix = df.corr(numeric_only=True)\n",
        "\n",
        "# 2. Análisis de contingencia (Categórica vs Categórica)\n",
        "contingencia = pd.crosstab(df['cat1'], df['cat2'])\n",
        "\n",
        "# 3. Segmentación (Categórica vs Numérica)\n",
        "segmentos = df.groupby('cat')['num'].describe()"
      ],
      "id": "32518ae9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformaciones y Multivariante"
      ],
      "id": "bbb2f936-2730-4998-915e-ab359dec55eb"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Transformación Logarítmica (reduce asimetría)\n",
        "df['log_val'] = np.log1p(df['val'])\n",
        "\n",
        "# 2. Agregaciones multivariante\n",
        "multi_stats = df.pivot_table(values='num', index='cat1', columns='cat2', aggfunc='mean')"
      ],
      "id": "480c8bbb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conceptos Clave\n",
        "\n",
        "-   **Asimetría (Skewness):** Medida que indica si los datos están\n",
        "    distribuidos de forma equilibrada respecto a la media o si se\n",
        "    concentran en un extremo.\n",
        "-   **Curtosis:** Medida que indica el grado de concentración de los\n",
        "    datos alrededor de la zona central de la distribución\n",
        "    (“puntiagudez”).\n",
        "-   **Correlación:** Relación estadística entre dos variables que indica\n",
        "    cómo varía una al cambiar la otra (no implica causalidad).\n",
        "-   ***p-valor*:** valor que ayuda a determinar la significancia\n",
        "    estadística de un hallazgo; si es bajo, es poco probable que el\n",
        "    resultado sea azaroso.\n",
        "\n",
        "### Checklist de autoevaluación\n",
        "\n",
        "Antes de pasar a la práctica, asegúrate de que puedes:\n",
        "\n",
        "-   [ ] Interpretar qué indica una asimetría (skewness) positiva o\n",
        "    negativa.\n",
        "-   [ ] Diferenciar entre una correlación fuerte y una relación de\n",
        "    causalidad.\n",
        "-   [ ] Interpretar el *p-valor* para validar o rechazar una hipótesis.\n",
        "-   [ ] Visualizar la curtosis para entender la frecuencia de valores\n",
        "    extremos.\n",
        "\n",
        "## Fuentes y Lecturas Recomendadas\n",
        "\n",
        "**¿Quieres profundizar más?** Consulta la bibliografía detallada, los\n",
        "enlaces a la documentación oficial y los recursos de aprendizaje para\n",
        "esta unidad en el **Apéndice B: Fuentes y Lecturas Recomendadas** al\n",
        "final de este libro.\n",
        "\n",
        "### Reflexión final"
      ],
      "id": "59d0365b-acad-4b52-94a6-5b34c0fae1bf"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  }
}